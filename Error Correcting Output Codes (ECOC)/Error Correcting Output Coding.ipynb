{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error-Correcting Output Coding\n",
    "\n",
    "\n",
    "#### David Solans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Introduction\n",
    "Error-Correcting Output Codes(ECOC) is an ensemble method designed for\n",
    "multi-class classification problem. In multi-class classification problem, the task\n",
    "is to decide one label from $k > 2$ possible choices. \n",
    "\n",
    "For example, in digit recognition task, we need to map each hand written digit to one of $k = 10$ classes.\n",
    "Some algorithms, such as decision tree, naive bayes and neural network, can\n",
    "handle multi-class problem directly.\n",
    "ECOC is a meta method which combines many binary classifiers in order to\n",
    "solve the multi-class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](ecoc1.png)\n",
    "<p style=\"text-align: center;\">Figure 1: A 15 bit error-correcting output code for a ten-class problem</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 shows a 15 bit error-correcting output code for a ten-class problem.\n",
    "\n",
    "Each class is assigned a unique binary string of length 15. The string is also\n",
    "called a codeword. For example, class 2 has the codeword $100100011110101$.\n",
    "During training, one binary classifier is learned for each column. For example,\n",
    "for the first column, we build a binary classifier to separate ${0, 2, 4, 6, 8}$ from\n",
    "${1, 3, 5, 7, 9}$. Thus 15 binary classifiers are trained in this way. To classify a\n",
    "new data point x, all 15 binary classifiers are evaluated to obtain a 15-bit string.\n",
    "Finally, we choose the class whose codeword is closet to x’s output string as the\n",
    "predicted label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Theoretical Justification\n",
    "Notice that in the error-correcting output code, the rows have more bits than\n",
    "is necessary. $log_2 10 = 4$ is enough for representing 10 different classes. Using\n",
    "some redundant ”error-correcting” bits, we can tolerant some error introduced\n",
    "by finite training sample, poor choice of input features, and flaws in the training\n",
    "algorithm. Thus the system is more likely to recover from the errors. If the\n",
    "minimum Hamming distance between any pair of code words is d, then the code\n",
    "can correct at least $|\\frac{d−1}{2}|$ single bit errors. As long as the error moves us fewer\n",
    "than $|\\frac{d−1}{2}|$ unit away from the true codeword, the nearest codeword is still the\n",
    "correct one. The code in Figure 1 can correct up to 3 errors out of the 15 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Code design\n",
    "There are many ways to design the error-correcting output code. Here we only\n",
    "introduce the two simplest ones.\n",
    "When the number of classes k is small $(3 < k ≤ 7)$, we can use exhaustive\n",
    "codes. Each code has length $2^{k−1}−1$. \n",
    "\n",
    "- Row 1 contains only ones. \n",
    "\n",
    "- Row 2 consists\n",
    "of $2^{k−2}$ zeros followed by $2^{k−2} − 1$ ones.\n",
    "\n",
    "- Row 3 consists of $2^{k−3}$ zeros, followed\n",
    "by $2^{k−3}$ ones, followed by $2^{k−3}$\n",
    "zeros, followed by $2^{k−3} −1$ ones. \n",
    "\n",
    "Figure 2 shows\n",
    "the exhaustive code for a five-class problem. The code has inter-row Hamming\n",
    "distance 8.\n",
    "When the number of classes k is large, random codes can be used. The major benefit of error-corrective coding is variance reduction via model averaging. Random code works as well as optimally\n",
    "constructed code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](ecoc2.png)\n",
    "<p style=\"text-align: center;\">Figure 2: Exhaustive code for a five class problem</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Working with ECOC in python\n",
    "\n",
    "We will use the class:\n",
    "<br>\n",
    "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OutputCodeClassifier.html\"> sklearn.multiclass.OutputCodeClassifier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OutputCodeClassifier, the `code_size` attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.\n",
    "\n",
    "A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, `log2(n_classes) / n_classes` is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since `log2(n_classes)` is much smaller than n_classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = OutputCodeClassifier(LinearSVC(random_state=0),\n",
    "                           code_size=2, random_state=0)\n",
    "clf.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
